{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4341dca2",
   "metadata": {},
   "source": [
    "### Natural Language Processing WORK TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6fc03129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/jiten/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0fa6f",
   "metadata": {},
   "source": [
    "Rules:\n",
    "\n",
    "`Punctuations` should be separated but NOT discarded.  Also:\n",
    "- You only separate punctuations that occurred only at the leading or trailing positions of a word.  If a punctuation (or punctuations) occur(s) within a word, you do not separate the word.  For example, \"\\\\$3.19\" should be separated into \"\\\\$\" and \"3.19\".  If NLTK word_tokenize() didn't adhere to it for any instance, do NOT take NLTK's tokenization and override yourself by separating ONLY the leading and the trailing punctuations.\n",
    "\n",
    "- If a consecutive (leading and trailing) punctuations were tokenized as one token (e.g. \"...\",  \"--\") by NLTK word_tokenize(), treat them as one token/word.\n",
    "\n",
    "- Or if a consecutive (leading and trailing) punctuations were tokenized as multiple tokens (e.g. \"!\", \"!\" from \"!!\") by NLTK word_tokenize(), treat them as multiple tokens (i.e, as is).\n",
    "\n",
    "- Also if a word ends with a period (.), you don't have to check if it is a known acronym (e.g. \"mr.\", \"m.p.g.\", \"e.g.\", \"etc.\").  Just use NLTK's tokenization/split.\n",
    " \n",
    "`Contractions` must be expanded and converted to the root/lemma tokens.  Although some contractions are ambiguous (e.g. \"they'd\" could be \"they would\" or \"they had\"), in this assignment you can make these simplifying assumptions.\n",
    "\n",
    "NLTK word_tokenize() splits contractions at the quote character (') or special cases such as \"n't\".  For example, it splits \"they'd\" to two tokens 'they' and \"'d\" , and \"don't\" to 'do' and  \"n't\".\n",
    "\n",
    "Contractions to convert:\n",
    "\n",
    "- \"n't\" -- assume \"not\" for all instances (e.g. \"don't\" -> \"do\" and \"not\"), EXCEPT for these special cases (where you'll need to look at the preceding token to determine):\n",
    "- won't -- \"will\" and \"not\"\n",
    "- can't -- \"can\" and \"not\"\n",
    "- shanâ€™t -- \"shall\" and \"not\"\n",
    "- \"'ll\" -- assume \"will\" for all instances; e.g. \"they'll\" -> \"they\" and \"will\"\n",
    "- \" 've\" -- assume \"have\" for all instances; e.g. \"they've\" -> \"they\" and \"have\"\n",
    "- \"'d\" -- assume \"would\" for all instances; e.g. \"they'd\" -> \"they\" and \"would\"\n",
    "- \"'re\" -- assume \"are\" for all instances; e.g. \"they're\" -> \"they\" and \"are\"\n",
    "- \"'s \" -- assume possessive (i.e., an apostrophe-s); e.g. \"phone's\" -> \"phone\" and \"'s\" ==> thus no change, EXCEPT for these special cases:\n",
    "    - \"let's\" -- \"let\" and \"us\"\n",
    "    - obvious contraction of \"is\" (mostly used with a singular pronoun or a wh-word; e.g. \"it's\" -> \"it\" and \"is\").  In this assignment, apply this rule to these words: \"he's\", \"she's\", \"it's\", \"that's\", \"here's\" and \"there's\", \"what's\", \"when's\", \"where's\", \"which's\", \"who's\" and \"how's\". \n",
    "- other special cases:\n",
    "    - i'm -- \"i\" and \"am\".\n",
    "\n",
    "Note that, if a word contains multiple contractions (e.g. \"shouldn't've\"), you must separate ALL of them (e.g. \"should\", \"not\", \"have\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "7295e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(lines):\n",
    "    \n",
    "    # variable to hold the string\n",
    "    data = \"\"\n",
    "    paragraphs = 0\n",
    "    in_paragraph = False\n",
    "\n",
    "    # loop through the lines and create the string\n",
    "    # make it lower case\n",
    "    for line in lines:\n",
    "        line_strip = line.strip()\n",
    "        if line_strip != '':\n",
    "            data = data + line_strip.lower() + \" \"\n",
    "            if not in_paragraph:\n",
    "                # Start of a new paragraph\n",
    "                paragraphs += 1\n",
    "                in_paragraph = True\n",
    "        else:\n",
    "            in_paragraph = False\n",
    "                \n",
    "    \n",
    "    data = data.strip()\n",
    "    return data, paragraphs\n",
    "\n",
    "\n",
    "# contraction dictionary to map to the rules\n",
    "# contractions_dict = {\n",
    "#     \"n't\": \"not\", \"'ll\": \"will\", \"'ve\": \"have\", \"'d\": \"would\", \"'re\": \"are\", \n",
    "#     \"i'm\": \"i am\", \"won't\": \"will not\", \n",
    "#     \"can't\": \"can not\", \"shan't\": \"shall not\"\n",
    "# }\n",
    "\n",
    "contractions_dict = {\n",
    "    \"n't\": \"not\", \"'ll\": \"will\", \"'ve\": \"have\", \"'d\": \"would\", \"'re\": \"are\", \"'m\": \"am\",\n",
    "    \"'s\": \"'s\", \"let's\": \"let us\", \"i'm\": \"i am\", \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\", \"can't\": \"can not\", \"shan't\": \"shall not\", \n",
    "    \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\", \"that's\": \"that is\", \n",
    "    \"here's\": \"here is\", \"there's\": \"there is\", \"what's\": \"what is\", \n",
    "    \"when's\": \"when is\", \"where's\": \"where is\", \"which's\": \"which is\", \n",
    "    \"who's\": \"who is\", \"how's\": \"how is\"\n",
    "}\n",
    "\n",
    "# Function to handle contraction expansion\n",
    "# if the toke is in contraction dict then epand it as per the mapping\n",
    "def expand_contractions(tokens):\n",
    "    expanded_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == \"n't\" and expanded_tokens and expanded_tokens[-1] in [\"do\", \"does\", \"did\", \"is\", \"are\", \n",
    "                                                                        \"was\", \"were\", \"has\", \"have\", \"had\", \n",
    "                                                                        \"ca\", \"would\", \"should\", \"could\", 'wo', 'sha']:\n",
    "            # won't will be converted to will not\n",
    "            # and shan't will be converted to shall not\n",
    "            if expanded_tokens[-1] == 'wo':\n",
    "                expanded_tokens[-1] = 'will'\n",
    "            if expanded_tokens[-1] == 'sha':\n",
    "                expanded_tokens[-1] = 'shall'\n",
    "            if expanded_tokens[-1] == 'ca':\n",
    "                expanded_tokens[-1] = 'can'\n",
    "                \n",
    "            expanded_tokens.append(\"not\")\n",
    "        elif token == \"'s\" and expanded_tokens and expanded_tokens[-1] in [\"let\"]:\n",
    "            # let's will be converted to let us\n",
    "            expanded_tokens.append(\"us\")\n",
    "        elif token == \"'s\" and expanded_tokens and expanded_tokens[-1] in [\"it\", \"that\", \"here\", \"there\", \"what\",\n",
    "                                                                          \"when\", \"where\", \"which\", \"who\", \"how\",\n",
    "                                                                          \"he\", \"she\"]:\n",
    "            # 's will be converted to is\n",
    "            expanded_tokens.append(\"is\")\n",
    "        elif token[0] == \"'\" and len(token) > 2 and token not in contractions_dict:\n",
    "            # anything that has a ' in the begining will be divided \n",
    "            # ex : 'have to ' and have\n",
    "            #print(token)\n",
    "            expanded_tokens.append(token[0])\n",
    "            expanded_tokens.append(token[1:])\n",
    "        elif token in contractions_dict:\n",
    "            expanded_tokens.extend(contractions_dict[token].split())\n",
    "        elif token.endswith(\".\") and len(token) > 1 and token[-1] != token[-2]:\n",
    "            #print(token) \n",
    "            expanded_tokens.append(token[:-1])\n",
    "            expanded_tokens.append(token[-1])\n",
    "        else:\n",
    "            expanded_tokens.append(token)\n",
    "    return expanded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "f53d0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFile(fileName, typ=None):\n",
    "    \n",
    "    # read the sample input file\n",
    "    if typ == 'utf-8':\n",
    "        f = open(fileName, 'r', encoding='utf-8')\n",
    "    else:\n",
    "        f = open(fileName, 'r')\n",
    "\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    # prepare the data and ge tthe paragraph details\n",
    "    data, paragraphs = getData(lines)\n",
    "    #print(f\"No of Paragraphs :: {paragraphs}\")\n",
    "    \n",
    "    # check the no of sentences\n",
    "    sentences = len(sent_tokenize(data))\n",
    "\n",
    "    #print(f\"Total no of sentences :: {sentences}\")\n",
    "    \n",
    "    # tokenize using word_tokenize\n",
    "    tokens = word_tokenize(data)\n",
    "    \n",
    "    # expand contraction\n",
    "    tokens_c = expand_contractions(tokens)\n",
    "    \n",
    "    # get the total no of tokens\n",
    "    total_tokens = len(tokens_c)\n",
    "    \n",
    "    # total unique tokens\n",
    "    unique_tokens = len(np.unique(tokens_c))\n",
    "    \n",
    "    # preare the counter and sorted dictionary for output\n",
    "    word_counter = Counter(tokens_c)\n",
    "    \n",
    "    # Sort first alphabetically (linguistically), then by frequency in descending order\n",
    "    sorted_word_counter = sorted(word_counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    sorted_word_counter = dict(sorted_word_counter)\n",
    "    \n",
    "    return data, paragraphs, sentences, total_tokens, unique_tokens, sorted_word_counter, tokens, tokens_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "54f0dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeOutput(num_paragraphs, num_sentences, num_tokens, num_unique_tokens, sorted_word_counter, output_file):\n",
    "    \n",
    "    with open(output_file, 'w') as file:\n",
    "        # write summary\n",
    "        file.write(f\"# of paragraphs: {num_paragraphs}\\n\")\n",
    "        file.write(f\"# of sentences: {num_sentences}\\n\")\n",
    "        file.write(f\"# of tokens: {num_tokens}\\n\")\n",
    "        file.write(f\"# of unique tokens: {num_unique_tokens}\\n\")\n",
    "        file.write(\"=\" * 20 )\n",
    "    \n",
    "        # Print token frequencies\n",
    "        for i, (token, count) in enumerate(sorted_word_counter.items(), start=1):\n",
    "            file.write(f\"\\n{i}: {token}  {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "72fac3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, paragraphs, sentences, total_tokens, unique_tokens, sorted_word_counter, tokens, tokens_c = processFile(\n",
    "    \"sample_2024.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "5f75c0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here are some random sentences.  let\\'s parse them :)  that\\'ll be fun!! if you can really see the future, you could\\'ve prevented all of this! i just have to report a really beautiful example i heard on my favorite public radio station on feb. 16 during their recent fund drive. i\\'ve met a member of their developmember staff and been interviewed by her, which\\'s to say it\\'s awesome.  i won\\'t name her simply because she might be \"embarrassed,\" and i wouldn\\'t want to cause that. this powerful true story of human decency and courage, so rare in the tragic annals of world war ii, is badly spoiled by terrible writing. many of the reviews here explain just what makes the writing so bad (e.g., john sollami\\'s 3-star review and comments on it) so i shan\\'t and won\\'t do that here.  it\\'s repetitive, slow, not interesting and i think if i have to read the word \"saumensch\" or \"saukerl\" one more time, i\\'ll be done with this book forever.  because you haven\\'t sopped up enough of that yet in your life, right? by far my favorite book! the language is beautiful and imaginative, and you can\\'t help but fall in love with the characters.  you have to have studied ww2 extensively along with the love for fiction and the struggle theme that\\'s projected through this whole book.  little do they know he has fled to london and they\\'re forced to stay with marie\\'s great uncle etienne where they remain hidden. intriguing story. i love how the book is arranged: parallel narration, third-person perspective, chapter-by-chapter with a changing view brought by new characters of what\\'s happening along the theme.  the characters are all connected yet seemingly disconnected.  it\\'s like watching a movie with the reader as the camera: we know all the sides but we don\\'t know how it is going. it keeps the reader engaged. definitely a page-turner. where\\'s the thriller?!!  he drives slow, uses a shoehorn, doesn\\'t know about hybrid cars, disagrees with using credit cards for shopping, doesn\\'t use a coffee maker because a percolator is perfectly fine, etc. i\\'m sorry but age 59 just doesn\\'t fit the story.  ove comes to like the cat and suddenly they\\'re best friends. at least the story\\'s predictability saved me from fretting incessantly about the cat. she\\'s a take-no-prisoners fireball. he\\'s a sweet-talking charmer. it could be love... if they don\\'t kill each other first. enemies to lovers romance. it wouldn\\'t have (wouldn\\'t\\'ve or had?) bothered me if it only appeared in dialogue, but i recall it occured several times in the narrative (by .... i started this comment to say that the \\'spelling correction\\' explanation seemed less likely in examples with things separating the \\'of\\' and \\'have\\').'"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the sample data to be processed\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "31bd7dc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# write the data to the output file\n",
    "writeOutput(paragraphs, sentences, total_tokens, unique_tokens, sorted_word_counter, 'output1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "a605d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of paragraphs: 8\n",
      "# of sentences: 31\n",
      "# of tokens: 567\n",
      "# of unique tokens: 274\n",
      "====================\n",
      "1: .  27\n",
      "2: the  24\n",
      "3: ,  19\n",
      "4: is  14\n",
      "5: not  14\n"
     ]
    }
   ],
   "source": [
    "# verify the file\n",
    "f = open('output1.txt')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "# Print only the first 10 lines\n",
    "for i, line in enumerate(lines):\n",
    "    if i < 10:\n",
    "        print(line.strip())\n",
    "    else:\n",
    "        break  # Stop after printing 10 lines\n",
    "\n",
    "# Close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "0e91d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, paragraphs, sentences, total_tokens, unique_tokens, sorted_word_counter, tokens, tokens_c = processFile(\n",
    "    \"war-and-peace.txt\", 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "f31c4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to the output file\n",
    "writeOutput(paragraphs, sentences, total_tokens, unique_tokens, sorted_word_counter, 'output2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "5b5d236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of paragraphs: 12169\n",
      "# of sentences: 31911\n",
      "# of tokens: 673101\n",
      "# of unique tokens: 18395\n",
      "====================\n",
      "1: ,  39882\n",
      "2: the  34717\n",
      "3: .  24992\n",
      "4: and  22289\n",
      "5: to  16750\n"
     ]
    }
   ],
   "source": [
    "# verify the file\n",
    "f = open('output2.txt')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "# Print only the first 10 lines\n",
    "for i, line in enumerate(lines):\n",
    "    if i < 10:\n",
    "        print(line.strip())\n",
    "    else:\n",
    "        break  # Stop after printing 10 lines\n",
    "\n",
    "# Close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "116ed653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4415"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_c.count('is')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
